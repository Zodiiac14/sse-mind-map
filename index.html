<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Mind Map</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="styles.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Mind Map</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#uvod-i-motivacija" id="toc-uvod-i-motivacija"><span
class="toc-section-number">1</span> Uvod i motivacija</a></li>
<li><a href="#nemv-nepomeren-estimator-minimalne-varijanse"
id="toc-nemv-nepomeren-estimator-minimalne-varijanse"><span
class="toc-section-number">2</span> NEMV – Nepomeren Estimator Minimalne
Varijanse</a></li>
<li><a href="#crdg-cramérrao-donja-granica"
id="toc-crdg-cramérrao-donja-granica"><span
class="toc-section-number">3</span> CRDG – Cramér–Rao Donja Granica</a>
<ul>
<li><a href="#fišerova-informacija" id="toc-fišerova-informacija"><span
class="toc-section-number">3.1</span> Fišerova informacija</a></li>
<li><a href="#uslov-regularnosti" id="toc-uslov-regularnosti"><span
class="toc-section-number">3.2</span> Uslov regularnosti</a></li>
<li><a href="#crdg-teorema" id="toc-crdg-teorema"><span
class="toc-section-number">3.3</span> CRDG – Teorema</a></li>
<li><a href="#signali-sn-theta-u-belom-gaussovom-šumu"
id="toc-signali-sn-theta-u-belom-gaussovom-šumu"><span
class="toc-section-number">3.4</span> Signali <span
class="math inline">s[n; \theta]</span> u belom Gaussovom šumu</a></li>
<li><a href="#crdg-za-funkciju-parametra"
id="toc-crdg-za-funkciju-parametra"><span
class="toc-section-number">3.5</span> CRDG za funkciju
parametra</a></li>
<li><a href="#konzistentnost-i-asimptotske-osobine"
id="toc-konzistentnost-i-asimptotske-osobine"><span
class="toc-section-number">3.6</span> Konzistentnost i asimptotske
osobine</a></li>
<li><a href="#fišerova-matrica-informacije"
id="toc-fišerova-matrica-informacije"><span
class="toc-section-number">3.7</span> Fišerova matrica
informacije</a></li>
<li><a href="#crdg-za-vektorski-parametar"
id="toc-crdg-za-vektorski-parametar"><span
class="toc-section-number">3.8</span> CRDG za vektorski
parametar</a></li>
</ul></li>
<li><a href="#linearni-gaussovski-modeli"
id="toc-linearni-gaussovski-modeli"><span
class="toc-section-number">4</span> Linearni Gaussovski Modeli</a>
<ul>
<li><a href="#efikasan-estimator-u-lg-modelu"
id="toc-efikasan-estimator-u-lg-modelu"><span
class="toc-section-number">4.1</span> Efikasan estimator u LG
modelu</a></li>
<li><a href="#primeri-primene-lg-modela"
id="toc-primeri-primene-lg-modela"><span
class="toc-section-number">4.2</span> Primeri primene LG modela</a></li>
<li><a href="#uslov-invertibilnosti"
id="toc-uslov-invertibilnosti"><span
class="toc-section-number">4.3</span> Uslov invertibilnosti</a></li>
<li><a href="#obojeni-šum" id="toc-obojeni-šum"><span
class="toc-section-number">4.4</span> Obojeni šum</a></li>
<li><a href="#posebni-slučajevi" id="toc-posebni-slučajevi"><span
class="toc-section-number">4.5</span> Posebni slučajevi</a></li>
</ul></li>
<li><a href="#nlne-najbolji-linearan-nepomeren-estimator-blue"
id="toc-nlne-najbolji-linearan-nepomeren-estimator-blue"><span
class="toc-section-number">5</span> NLNE – Najbolji linearan nepomeren
estimator (BLUE)</a>
<ul>
<li><a href="#motivacija" id="toc-motivacija"><span
class="toc-section-number">5.1</span> Motivacija</a></li>
<li><a href="#uslov-nepomerenosti-skalarni-slučaj"
id="toc-uslov-nepomerenosti-skalarni-slučaj"><span
class="toc-section-number">5.2</span> Uslov nepomerenosti (skalarni
slučaj)</a></li>
<li><a href="#nalaženje-nlne-skalarni-slučaj"
id="toc-nalaženje-nlne-skalarni-slučaj"><span
class="toc-section-number">5.3</span> Nalaženje NLNE (skalarni
slučaj)</a></li>
<li><a href="#kada-nlne-nije-adekvatan"
id="toc-kada-nlne-nije-adekvatan"><span
class="toc-section-number">5.4</span> Kada NLNE nije adekvatan?</a></li>
<li><a href="#vektorski-slučaj-blue-za-boldsymbolthetainmathbbrp"
id="toc-vektorski-slučaj-blue-za-boldsymbolthetainmathbbrp"><span
class="toc-section-number">5.5</span> Vektorski slučaj (BLUE za <span
class="math inline">\boldsymbol{\theta}\in\mathbb{R}^p</span>)</a></li>
</ul></li>
<li><a href="#estimator-maksimalne-verodostojnosti-emv-mle"
id="toc-estimator-maksimalne-verodostojnosti-emv-mle"><span
class="toc-section-number">6</span> Estimator maksimalne verodostojnosti
(EMV / MLE)</a>
<ul>
<li><a href="#definicija-i-motivacija"
id="toc-definicija-i-motivacija"><span
class="toc-section-number">6.1</span> Definicija i motivacija</a></li>
<li><a href="#primeri" id="toc-primeri"><span
class="toc-section-number">6.2</span> Primeri</a></li>
<li><a href="#osobine-emv" id="toc-osobine-emv"><span
class="toc-section-number">6.3</span> Osobine EMV</a></li>
<li><a href="#numerička-maksimizacija-verodostojnosti"
id="toc-numerička-maksimizacija-verodostojnosti"><span
class="toc-section-number">6.4</span> Numerička maksimizacija
verodostojnosti</a></li>
</ul></li>
<li><a href="#bayesovska-filozofija-i-emskg-posteriorna-sredina"
id="toc-bayesovska-filozofija-i-emskg-posteriorna-sredina"><span
class="toc-section-number">7</span> Bayesovska filozofija i EMSKG
(posteriorna sredina)</a>
<ul>
<li><a href="#bayesovska-filozofija"
id="toc-bayesovska-filozofija"><span
class="toc-section-number">7.1</span> Bayesovska filozofija</a></li>
<li><a href="#bayesovska-srednje-kvadratna-greška-bskg"
id="toc-bayesovska-srednje-kvadratna-greška-bskg"><span
class="toc-section-number">7.2</span> Bayesovska srednje-kvadratna
greška (BSKG)</a></li>
<li><a href="#estimator-minimalne-srednje-kvadratne-greške-emskg"
id="toc-estimator-minimalne-srednje-kvadratne-greške-emskg"><span
class="toc-section-number">7.3</span> Estimator minimalne
srednje-kvadratne greške (EMSKG)</a></li>
<li><a href="#primer-konstanta-u-belom-gaussovom-šumu"
id="toc-primer-konstanta-u-belom-gaussovom-šumu"><span
class="toc-section-number">7.4</span> Primer: konstanta u belom
Gaussovom šumu</a></li>
<li><a href="#izbor-apriorne-fgv" id="toc-izbor-apriorne-fgv"><span
class="toc-section-number">7.5</span> Izbor apriorne FGV</a></li>
<li><a href="#bayesovski-vs.-frekventistički-pristup"
id="toc-bayesovski-vs.-frekventistički-pristup"><span
class="toc-section-number">7.6</span> Bayesovski vs. frekventistički
pristup</a></li>
<li><a href="#osobine-gaussove-fgv" id="toc-osobine-gaussove-fgv"><span
class="toc-section-number">7.7</span> Osobine Gaussove FGV</a></li>
<li><a href="#bayesovski-linearan-model"
id="toc-bayesovski-linearan-model"><span
class="toc-section-number">7.8</span> Bayesovski linearan model</a></li>
<li><a href="#alternativne-forme-za-c_thetax-i-mu_thetax"
id="toc-alternativne-forme-za-c_thetax-i-mu_thetax"><span
class="toc-section-number">7.9</span> Alternativne forme za <span
class="math inline">C_{\theta|x}</span> i <span
class="math inline">\mu_{\theta|x}</span></a></li>
<li><a href="#dodatna-pitanja" id="toc-dodatna-pitanja"><span
class="toc-section-number">7.10</span> Dodatna pitanja</a></li>
</ul></li>
<li><a href="#opšta-bayesovska-estimacija"
id="toc-opšta-bayesovska-estimacija"><span
class="toc-section-number">8</span> Opšta Bayesovska estimacija</a>
<ul>
<li><a href="#osnovni-pojmovi" id="toc-osnovni-pojmovi"><span
class="toc-section-number">8.1</span> Osnovni pojmovi</a></li>
<li><a href="#osobine-emskg" id="toc-osobine-emskg"><span
class="toc-section-number">8.2</span> Osobine EMSKG</a></li>
<li><a href="#osobine-mape" id="toc-osobine-mape"><span
class="toc-section-number">8.3</span> Osobine MAPE</a></li>
</ul></li>
<li><a href="#linearna-bayesovska-estimacija-lbe"
id="toc-linearna-bayesovska-estimacija-lbe"><span
class="toc-section-number">9</span> Linearna Bayesovska estimacija
(LBE)</a>
<ul>
<li><a href="#definicija-i-klasa-linearnog-estimatora"
id="toc-definicija-i-klasa-linearnog-estimatora"><span
class="toc-section-number">9.1</span> Definicija i klasa linearnog
estimatora</a></li>
<li><a href="#optimalni-koeficijenti-lemskg"
id="toc-optimalni-koeficijenti-lemskg"><span
class="toc-section-number">9.2</span> Optimalni koeficijenti
LEMSKG</a></li>
<li><a href="#geometrijska-interpretacija"
id="toc-geometrijska-interpretacija"><span
class="toc-section-number">9.3</span> Geometrijska
interpretacija</a></li>
<li><a href="#princip-ortogonalnosti"
id="toc-princip-ortogonalnosti"><span
class="toc-section-number">9.4</span> Princip ortogonalnosti</a></li>
<li><a href="#vektorski-lemskg" id="toc-vektorski-lemskg"><span
class="toc-section-number">9.5</span> Vektorski LEMSKG</a></li>
</ul></li>
<li><a href="#kalmanov-filter-kf" id="toc-kalmanov-filter-kf"><span
class="toc-section-number">10</span> Kalmanov filter (KF)</a></li>
<li><a href="#pregled-i-povezanost-formula"
id="toc-pregled-i-povezanost-formula"><span
class="toc-section-number">11</span> Pregled i povezanost formula</a>
<ul>
<li><a href="#verodostojnost-i-log-verodostojnost"
id="toc-verodostojnost-i-log-verodostojnost"><span
class="toc-section-number">11.1</span> Verodostojnost i
log-verodostojnost</a></li>
<li><a href="#fišerova-informacija-1"
id="toc-fišerova-informacija-1"><span
class="toc-section-number">11.2</span> Fišerova informacija</a></li>
<li><a href="#crdg-cramérrao-donja-granica-1"
id="toc-crdg-cramérrao-donja-granica-1"><span
class="toc-section-number">11.3</span> CRDG – Cramér–Rao Donja
Granica</a></li>
<li><a href="#nlne-blue-best-linear-unbiased-i-lemskg-linearni-mmse"
id="toc-nlne-blue-best-linear-unbiased-i-lemskg-linearni-mmse"><span
class="toc-section-number">11.4</span> NLNE / BLUE (best linear
unbiased) i LEMSKG (linearni MMSE)</a></li>
<li><a href="#linearni-gaussovski-lg-bayes-model-posterior"
id="toc-linearni-gaussovski-lg-bayes-model-posterior"><span
class="toc-section-number">11.5</span> Linearni Gaussovski (LG) Bayes
model – posterior</a></li>
<li><a href="#mle-emv" id="toc-mle-emv"><span
class="toc-section-number">11.6</span> MLE (EMV)</a></li>
<li><a href="#bayes-posterior-rizik-i-optimalni-estimatori"
id="toc-bayes-posterior-rizik-i-optimalni-estimatori"><span
class="toc-section-number">11.7</span> Bayes: posterior, rizik i
optimalni estimatori</a></li>
<li><a href="#veze-između-pristupa-blue-mvue-bayes-mle"
id="toc-veze-između-pristupa-blue-mvue-bayes-mle"><span
class="toc-section-number">11.8</span> Veze između pristupa (BLUE, MVUE,
Bayes, MLE)</a></li>
</ul></li>
</ul>
</nav>
<h2 data-number="1" id="uvod-i-motivacija"><span
class="header-section-number">1</span> Uvod i motivacija</h2>
<details class="card">
<summary>
<strong>Šta je model u teoriji estimacije?</strong>
</summary>
<div class="card-body">
<p>Model u teoriji estimacije je f.g.v. za opservacije, pri čemu u <span
class="math inline">p</span> figuriše <span
class="math inline">\theta</span>.</p>
</div></details>
<details class="card">
<summary>
<strong>Kada kažemo da je estimator nepomeren?</strong>
</summary>
<div class="card-body">
<p>Kažemo da je estimator nepomeren kada u proseku daje tačnu vrednost
<span class="math inline">E[\hat{\theta}] = \theta</span>.</p>
</div></details>
<details class="card">
<summary>
<strong>Šta je pomeraj estimatora?</strong>
</summary>
<div class="card-body">
<p>Pomeraj estimatora ukazuje na sistematsku grešku: <span
class="math inline">b(\theta) = E(\hat{\theta}) - \theta</span>.</p>
</div></details>
<details class="card">
<summary>
<strong>Šta je srednja kvadratna greška estimatora?</strong>
</summary>
<div class="card-body">
<p><span class="math inline">\operatorname{mse}(\hat{\theta})=E\left[
(\hat{\theta} - \theta)^2
\right]=\operatorname{var}(\hat{\theta})+b^2(\theta)</span>. </p>
</div></details>
<hr />
<h2 data-number="2"
id="nemv-nepomeren-estimator-minimalne-varijanse"><span
class="header-section-number">2</span> NEMV – Nepomeren Estimator
Minimalne Varijanse</h2>
<details class="card">
<summary>
<strong>Da li je moguće odrediti EMSKG u frekventističkom
pristupu?</strong>
</summary>
<div class="card-body">
<p>Nije moguće odrediti estimator minimalne srednje kvadratne greške u
frekventističkom pristupu. Ako tražimo <strong>nepomeren</strong>
estimator, želimo onaj sa <strong>najmanjom varijansom</strong>.</p>
</div></details>
<ul>
<li><strong>Koraci:</strong>
<ol type="1">
<li>Definiši klasu nepomerenih estimatora (uslov <span
class="math inline">E\hat{\theta}=\theta</span>).</li>
<li>Nađi onaj sa minimalnom varijansom; kada postoji i kada ne
postoji.</li>
</ol></li>
<li><strong>Zašto:</strong> NEMV je „zlatni standard“ u klasi
nepomerenih estimatora; često služi kao referenca (donja granica).</li>
</ul>
<hr />
<h2 data-number="3" id="crdg-cramérrao-donja-granica"><span
class="header-section-number">3</span> CRDG – Cramér–Rao Donja
Granica</h2>
<details class="card">
<summary>
<strong>Šta je verodostojnost?</strong>
</summary>
<div class="card-body">
<p>Verodostojnost je funkcija gustine verovatnoće posmatrana kao
funkcija od <span class="math inline">\theta</span> za fiksno <span
class="math inline">x</span>:<br />
<span class="math inline">L(\theta) = p(x; \theta)</span>.<br />
Za razliku od FGV, ne mora biti normirana: <span
class="math inline">\int L(\theta) \, d\theta \neq 1</span> u opštem
slučaju.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda verodostojnost za <span class="math inline">x =
\theta w, \; w \sim \mathcal{N}(0, 1)</span>?</strong>
</summary>
<div class="card-body">
<p>Tada je <span class="math inline">x \sim \mathcal{N}(0,
\theta^2)</span> i:<br />
<span class="math inline">L(\theta) = \frac{1}{\sqrt{2\pi \theta^2}}
\exp\left( -\frac{x^2}{2\theta^2} \right)</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda verodostojnost za <span class="math inline">x \sim
U[0, \theta]</span>?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
L(\theta) =
\begin{cases}
\frac{1}{\theta}, &amp; \theta \geq x \\
0, &amp; \theta &lt; x
\end{cases}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Od čega zavisi tačnost estimatora u smislu
verodostojnosti?</strong>
</summary>
<div class="card-body">
<p>Od “zašiljenosti” funkcije verodostojnosti — uži i viši vrh znači
veću tačnost estimacije.</p>
</div></details>
<hr />
<h3 data-number="3.1" id="fišerova-informacija"><span
class="header-section-number">3.1</span> Fišerova informacija</h3>
<details class="card">
<summary>
<strong>Kako definišemo log-verodostojnost i Fišerovu
informaciju?</strong>
</summary>
<div class="card-body">
<p>Log-verodostojnost: <span class="math inline">\ell(\theta) = \ln p(x;
\theta)</span><br />
Fišerova informacija: <span class="math display">
I(\theta) = -E\left[ \frac{\partial^2}{\partial \theta^2} \ln p(x;
\theta) \right]
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Koja je intuicija iza Fišerove informacije?</strong>
</summary>
<div class="card-body">
<p>Ona meri prosečnu zakrivljenost log-verodostojnosti — što je kriva
“oštrija”, to imamo više informacije o <span
class="math inline">\theta</span>.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako se Fišerova informacija računa za nezavisne jednako
raspodeljene uzorke?</strong>
</summary>
<div class="card-body">
<p>Važi aditivnost:<br />
<span class="math inline">\ell(\theta) = \sum_{n=0}^{N-1} \ln p(x[n];
\theta)</span><br />
<span class="math inline">I(\theta) = \sum_{n=0}^{N-1}
I_n(\theta)</span></p>
</div></details>
<hr />
<h3 data-number="3.2" id="uslov-regularnosti"><span
class="header-section-number">3.2</span> Uslov regularnosti</h3>
<details class="card">
<summary>
<strong>Kako glasi uslov regularnosti CRDG?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
E\left[ \frac{\partial}{\partial \theta} \ln p(x; \theta) \right] = 0,
\quad \forall\theta
</span><br />
Ovaj izraz nazivamo Fišerov skor.</p>
</div></details>
<details class="card">
<summary>
<strong>Kada važi ekvivalencija dve definicije Fišerove
informacije?</strong>
</summary>
<div class="card-body">
<p>Ako je ispunjen uslov regularnosti i izvod i integral mogu da zamene
mesta, tada: <span class="math display">
I(\theta) = -E\left[ \frac{\partial^2}{\partial \theta^2} \ln p(x;
\theta) \right]
= E\left[ \left( \frac{\partial}{\partial \theta} \ln p(x; \theta)
\right)^2 \right]
</span></p>
</div></details>
<hr />
<h3 data-number="3.3" id="crdg-teorema"><span
class="header-section-number">3.3</span> CRDG – Teorema</h3>
<details class="card">
<summary>
<strong>Kako glasi Cramér–Rao donja granica za nepomereni
estimator?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\mathrm{var}(\hat{\theta}) \geq \frac{1}{I(\theta)}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kada postoji efikasan estimator koji dostiže CRDG?</strong>
</summary>
<div class="card-body">
<p>Ako postoji faktorizacija: <span class="math display">
\frac{\partial}{\partial \theta} \ln p(x; \theta) = i(\theta) \, [g(x) -
\theta]
</span> tada je <span class="math inline">\hat{\theta}_{\mathrm{NEMV}} =
g(x)</span> efikasan i: <span class="math display">
\mathrm{var}(\hat{\theta}_{\mathrm{NEMV}}) = \frac{1}{I(\theta)}
</span></p>
</div></details>
<hr />
<h3 data-number="3.4" id="signali-sn-theta-u-belom-gaussovom-šumu"><span
class="header-section-number">3.4</span> Signali <span
class="math inline">s[n; \theta]</span> u belom Gaussovom šumu</h3>
<details class="card">
<summary>
<strong>Kako izgleda model signala u belom Gaussovom šumu?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
x[n] = s[n; \theta] + w[n], \quad n = 0, 1, \dots, N-1
</span><br />
gde je <span class="math inline">w[n] \sim \mathcal{N}(0,
\sigma^2)</span> – beli Gaussov šum.</p>
</div></details>
<details class="card">
<summary>
<strong>Fišerova informacija za signale u belom Gaussovom šumu:</strong>
</summary>
<div class="card-body">
<p><span class="math display">
I(\theta) = \frac{1}{\sigma^2} \sum_{n=0}^{N-1} \left( \frac{\partial
s[n; \theta]}{\partial \theta} \right)^2
</span></p>
</div></details>
<hr />
<h3 data-number="3.5" id="crdg-za-funkciju-parametra"><span
class="header-section-number">3.5</span> CRDG za funkciju parametra</h3>
<details class="card">
<summary>
<strong>Kako glasi CRDG za <span class="math inline">\alpha =
a(\theta)</span>?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\mathrm{var}(\hat{\alpha}) \geq \frac{\left( \frac{da(\theta)}{d\theta}
\right)^2}{I(\theta)}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kada je efikasnost očuvana kod transformacije
estimatora?</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">\hat{\theta}</span> efikasan
estimator za <span class="math inline">\theta</span>, onda je <span
class="math inline">\hat{\alpha} = a(\hat{\theta})</span> takođe
efikasan <strong>ako</strong> transformacija ima oblik:<br />
<span class="math display">
a(\theta) = c\,\theta + d
</span><br />
U tom slučaju: <span class="math display">
\mathrm{var}(\hat{\alpha}) = c^2\,\mathrm{var}(\hat{\theta}) =
\frac{c^2}{I(\theta)}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kada efikasnost nije očuvana?</strong>
</summary>
<div class="card-body">
<p>Ako <span class="math inline">a(\theta)</span> nije linearna funkcija
parametra, efikasnost u opštem slučaju <strong>nije</strong>
očuvana.<br />
Primer: <span class="math inline">A</span> u belom Gaussovom šumu —
<span class="math inline">\hat{A} = \bar{x}</span> efikasan, ali <span
class="math inline">\hat{\alpha} = \bar{x}^2</span> nije ni nepomeren ni
efikasan estimator za <span class="math inline">\alpha = A^2</span>.</p>
</div></details>
<hr />
<h3 data-number="3.6" id="konzistentnost-i-asimptotske-osobine"><span
class="header-section-number">3.6</span> Konzistentnost i asimptotske
osobine</h3>
<details class="card">
<summary>
<strong>Definicija konzistentnog estimatora</strong>
</summary>
<div class="card-body">
<p>Estimator <span class="math inline">\hat{\theta}</span> je
konzistentan ako: <span class="math display">
\lim_{N \to \infty} \mathrm{var}(\hat{\theta}) = 0
</span> odnosno, <span class="math inline">\hat{\theta} \xrightarrow{P}
\theta</span> kada <span class="math inline">N</span> raste.</p>
</div></details>
<details class="card">
<summary>
<strong>Definicija asimptotski nepomerenog estimatora</strong>
</summary>
<div class="card-body">
<p>Estimator <span class="math inline">\hat{\theta}</span> je
asimptotski nepomeren ako: <span class="math display">
\lim_{N \to \infty} E[\hat{\theta}] = \theta
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Definicija asimptotski efikasnog estimatora</strong>
</summary>
<div class="card-body">
<p>Estimator <span class="math inline">\hat{\theta}</span> je
asimptotski efikasan ako: <span class="math display">
\lim_{N \to \infty} \mathrm{var}(\hat{\theta}) = \frac{1}{I(\theta)}
</span> odnosno, za veliko <span class="math inline">N</span> dostiže
CRDG.</p>
</div></details>
<hr />
<h3 data-number="3.7" id="fišerova-matrica-informacije"><span
class="header-section-number">3.7</span> Fišerova matrica
informacije</h3>
<details class="card">
<summary>
<strong>Šta je Fišerova matrica informacije?</strong>
</summary>
<div class="card-body">
<p>Fišerova matrica informacije je proširenje Fišerove informacije na
vektorski parametar <span class="math inline">\theta</span>.<br />
Definiše se kao: <span class="math display">
[I(\theta)]_{ij} = -E\left[ \frac{\partial^2}{\partial \theta_i \partial
\theta_j} \ln p(x; \theta) \right]
</span> ili ekvivalentno (pod uslovom regularnosti): <span
class="math display">
[I(\theta)]_{ij} = E\left[ \frac{\partial}{\partial \theta_i} \ln p(x;
\theta) \cdot \frac{\partial}{\partial \theta_j} \ln p(x; \theta)
\right]
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda Fišerova matrica u punom obliku?</strong>
</summary>
<div class="card-body">
<p>Za <span class="math inline">\theta = [\theta_1, \theta_2, \dots,
\theta_p]^T</span>: <span class="math display">
I(\theta) =
\begin{bmatrix}
-\!E\!\left[ \frac{\partial^2 \ell}{\partial \theta_1^2} \right] &amp;
-\!E\!\left[ \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_2}
\right] &amp; \dots &amp; -\!E\!\left[ \frac{\partial^2 \ell}{\partial
\theta_1 \partial \theta_p} \right] \\
-\!E\!\left[ \frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_1}
\right] &amp; -\!E\!\left[ \frac{\partial^2 \ell}{\partial \theta_2^2}
\right] &amp; \dots &amp; -\!E\!\left[ \frac{\partial^2 \ell}{\partial
\theta_2 \partial \theta_p} \right] \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
-\!E\!\left[ \frac{\partial^2 \ell}{\partial \theta_p \partial \theta_1}
\right] &amp; -\!E\!\left[ \frac{\partial^2 \ell}{\partial \theta_p
\partial \theta_2} \right] &amp; \dots &amp; -\!E\!\left[
\frac{\partial^2 \ell}{\partial \theta_p^2} \right]
\end{bmatrix}
</span> gde je <span class="math inline">\ell(\theta) = \ln p(x;
\theta)</span>.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda Fišerova matrica za poznati model <span
class="math inline">x = A + w \sim \mathcal{N}(A, \sigma^2)</span> i
<span class="math inline">\theta = [A, \sigma^2]^T</span>?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\nabla_{\theta} \ln p(x; \theta) =
\begin{bmatrix}
\frac{x - \theta_1}{\theta_2} \\
-\frac{1}{2\theta_2} + \frac{(x - \theta_1)^2}{2\theta_2^2}
\end{bmatrix}
</span> Iz čega sledi: <span class="math display">
I(\theta) =
\begin{bmatrix}
\frac{1}{\theta_2} &amp; 0 \\
0 &amp; \frac{1}{2\theta_2^2}
\end{bmatrix}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Koji je uslov regularnosti za Fišerovu matricu?</strong>
</summary>
<div class="card-body">
<p>Za vektorski parametar <span class="math inline">\theta</span> uslov
regularnosti glasi: <span class="math display">
E\left[ \nabla_{\theta} \ln p(x; \theta) \right] = \mathbf{0}, \quad
\forall \theta
</span> odnosno: <span class="math display">
E\left[ \frac{\partial}{\partial \theta_i} \ln p(x; \theta) \right] = 0,
\quad \forall i
</span> Ovo znači da je očekivanje vektora Fišerovog skora jednako
nuli.</p>
</div></details>
<h3 data-number="3.8" id="crdg-za-vektorski-parametar"><span
class="header-section-number">3.8</span> CRDG za vektorski
parametar</h3>
<details class="card">
<summary>
<strong>Kako glasi Cramér–Rao Donja Granica za vektorski
parametar?</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">\hat{\theta}</span> nepomeren
estimator vektora <span class="math inline">\theta</span> i ako je
ispunjen uslov regularnosti, tada: <span class="math display">
\mathbf{C}_{\hat{\theta}} - I^{-1}(\theta) \geq 0
</span> gde je <span
class="math inline">\mathbf{C}_{\hat{\theta}}</span> kovarijaciona
matrica estimatora, a <span class="math inline">I(\theta)</span>
Fišerova matrica informacije.</p>
</div></details>
<details class="card">
<summary>
<strong>Kada je estimator efikasan u vektorskom slučaju?</strong>
</summary>
<div class="card-body">
<p>Estimator <span class="math inline">\hat{\theta} = g(x)</span> je
efikasan ako: <span class="math display">
\nabla_{\theta} \ln p(x; \theta) = I(\theta) \, [g(x) - \theta]
</span> i tada važi: <span class="math display">
\mathbf{C}_{\hat{\theta}} = I^{-1}(\theta)
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kako se CRDG transformiše kod promenljive <span
class="math inline">\alpha = a(\theta)</span>?</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">\alpha = a(\theta)</span> i <span
class="math inline">J(a(\theta)) = \frac{\partial a(\theta)}{\partial
\theta}</span> Jakobijan transformacije, tada: <span
class="math display">
\mathbf{C}_{\hat{\alpha}} - J(a(\theta)) \, I^{-1}(\theta) \,
J^T(a(\theta)) \geq 0
</span></p>
</div></details>
<hr />
<h2 data-number="4" id="linearni-gaussovski-modeli"><span
class="header-section-number">4</span> Linearni Gaussovski Modeli</h2>
<details class="card">
<summary>
<strong>Šta je linearni Gaussovski model?</strong>
</summary>
<div class="card-body">
<p>Model oblika: <span class="math display">
x = H\theta + w
</span> gde je:</p>
<ul>
<li><p><span class="math inline">x</span> – vektor opservacija dimenzije
<span class="math inline">N \times 1</span></p></li>
<li><p><span class="math inline">\theta</span> – vektor nepoznatih
parametara dimenzije <span class="math inline">p \times
1</span></p></li>
<li><p><span class="math inline">w</span> – beli Gaussov šum, <span
class="math inline">w \sim \mathcal{N}(0, \sigma^2 I)</span></p></li>
<li><p><span class="math inline">H</span> – poznata opservaciona matrica
<span class="math inline">N \times p</span></p></li>
</ul>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda matrica opservacije za primer modelovanja cene
akcije:</strong> <span class="math display">
x[n] = \theta_0 + \theta_1 n + w[n], \quad n = 0, \dots, N-1
</span>
</summary>
<div class="card-body">
<p><span class="math display">
H =
\begin{bmatrix}
1 &amp; 0 \\
\vdots &amp; \vdots \\
1 &amp; N-1
\end{bmatrix}
</span></p>
</div></details>
<hr />
<h3 data-number="4.1" id="efikasan-estimator-u-lg-modelu"><span
class="header-section-number">4.1</span> Efikasan estimator u LG
modelu</h3>
<details class="card">
<summary>
<strong>Koji je efikasan estimator u LG modelu?</strong>
</summary>
<div class="card-body">
<p>Ako je rang(<span class="math inline">H^T H</span>) = <span
class="math inline">p</span>, efikasan estimator je: <span
class="math display">
\hat{\theta} = (H^T H)^{-1} H^T x
</span> sa kovarijacionom matricom: <span class="math display">
C_{\hat{\theta}} = \sigma^2 (H^T H)^{-1}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Koja su osnovna pravila diferenciranja kod vektora i
matrica?</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">b</span> vektor i <span
class="math inline">A</span> simetrična matrica, važe: <span
class="math display">
\frac{\partial (b^T \theta)}{\partial \theta} = b, \quad
\frac{\partial (\theta^T A \theta)}{\partial \theta} = 2A\theta
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Šta znači uslov rang(<span class="math inline">H^T H</span>) =
p?</strong>
</summary>
<div class="card-body">
<p>To znači da je rang matrice <span class="math inline">H^T H</span>
jednak broju kolona matrice <span class="math inline">H</span>, tj. da
su kolone matrice <span class="math inline">H</span> linearno
nezavisne.<br />
Ovaj uslov je neophodan da bi <span class="math inline">(H^T
H)^{-1}</span> postojala.</p>
</div></details>
<details class="card">
<summary>
<strong>Šta je pseudo-inverzija matrice?</strong>
</summary>
<div class="card-body">
<p>Pseudo-inverzija (Moore–Penrose-ova inverzija) je generalizacija
inverza matrice koja postoji i kada matrica nije kvadratna ili nije
punog ranga.<br />
Ona omogućava rešavanje sistema najmanjih kvadrata kada <span
class="math inline">H</span> nije kvadratna, prema formuli: <span
class="math display">
H^+ = (H^T H)^{-1} H^T
</span> za slučaj kada <span class="math inline">H</span> ima pun rang
kolona.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda <span class="math inline">\hat{\theta}</span> kada
je <span class="math inline">H</span> kvadratna i invertibilna
matrica?</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">H</span> kvadratna (<span
class="math inline">N = p</span>) i invertibilna, tada estimator ima
oblik: <span class="math display">
\hat{\theta} = H^{-1} x
</span> što je tačno rešenje sistema <span class="math inline">x =
H\theta</span> bez potrebe za metodom najmanjih kvadrata.</p>
</div></details>
<hr />
<h3 data-number="4.2" id="primeri-primene-lg-modela"><span
class="header-section-number">4.2</span> Primeri primene LG modela</h3>
<p><strong>Kako izgleda matrica <span class="math inline">H</span> u
narednim primerima?</strong></p>
<details class="card">
<summary>
<strong>U linearnoj regresiji imamo model:</strong> <span
class="math display">
x[n] = \theta_0 + n\theta_1 + w[n], \quad n = 0, \dots, N-1
</span>
</summary>
<div class="card-body">
<p><span class="math display">
H =
\begin{bmatrix}
1 &amp; 0 \\
1 &amp; 1 \\
1 &amp; 2 \\
\vdots &amp; \vdots \\
1 &amp; N-1
\end{bmatrix}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>U polinomijalnoj regresiji reda <span
class="math inline">p</span> imamo model:</strong> <span
class="math display">
x[n] = \theta_0 + \theta_1 n + \dots + \theta_p n^p + w[n], \quad n = 0,
\dots, N-1
</span>
</summary>
<div class="card-body">
<p><span class="math display">
H =
\begin{bmatrix}
1 &amp; 0 &amp; 0^2 &amp; \dots &amp; 0^p \\
1 &amp; 1 &amp; 1^2 &amp; \dots &amp; 1^p \\
1 &amp; 2 &amp; 2^2 &amp; \dots &amp; 2^p \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; N-1 &amp; (N-1)^2 &amp; \dots &amp; (N-1)^p
\end{bmatrix}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kod identifikacije impulsnog odziva LTI sistema imamo
model:</strong> <span class="math display">
x[n] = \sum_{k=0}^{p-1} \theta[k]\, u[n-k] + w[n], \quad n = 0, \dots,
N-1
</span>
</summary>
<div class="card-body">
<p><span class="math display">
H =
\begin{bmatrix}
u[0] &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
u[1] &amp; u[0] &amp; 0 &amp; \dots &amp; 0 \\
u[2] &amp; u[1] &amp; u[0] &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
u[N-1] &amp; u[N-2] &amp; u[N-3] &amp; \dots &amp; u[N-p]
\end{bmatrix}
</span></p>
</div></details>
<hr />
<h3 data-number="4.3" id="uslov-invertibilnosti"><span
class="header-section-number">4.3</span> Uslov invertibilnosti</h3>
<details class="card">
<summary>
<strong>Koji je uslov invertibilnosti za LG model?</strong>
</summary>
<div class="card-body">
<p>Matrica <span class="math inline">H</span> mora imati rang <span
class="math inline">p</span> — kolone <span class="math inline">H</span>
moraju biti linearno nezavisne.</p>
</div></details>
<details class="card">
<summary>
<strong>Šta se dešava ako uslov invertibilnosti nije ispunjen?</strong>
</summary>
<div class="card-body">
<p>Ako kolone <span class="math inline">H</span> nisu linearno
nezavisne, rešenje po <span class="math inline">\theta</span> nije
jednoznačno čak i bez šuma.</p>
</div></details>
<hr />
<h3 data-number="4.4" id="obojeni-šum"><span
class="header-section-number">4.4</span> Obojeni šum</h3>
<details class="card">
<summary>
<strong>Šta je obojeni šum?</strong>
</summary>
<div class="card-body">
<p>Šum sa kovarijacionom matricom <span class="math inline">C \neq
\sigma^2 I</span>.<br />
<span class="math inline">w \sim \mathcal{N}(0, C)</span> sa <span
class="math inline">C &gt; 0</span>.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako se LG model transformiše u beli šum kod obojenog
šuma?</strong>
</summary>
<div class="card-body">
<p>Ako <span class="math inline">C^{-1} = D^T D</span> (npr. iz Cholesky
dekompozicije), definišemo: <span class="math display">
x&#39; = D x, \quad H&#39; = D H, \quad w&#39; = D w
</span> Tada važi: <span class="math display">
\hat{\theta} = (H^T C^{-1} H)^{-1} H^T C^{-1} x
</span></p>
</div></details>
<hr />
<h3 data-number="4.5" id="posebni-slučajevi"><span
class="header-section-number">4.5</span> Posebni slučajevi</h3>
<details class="card">
<summary>
<strong>Kako se procenjuje konstanta u nestacionarnom nekorelisnom
Gaussovom šumu?</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">x[n] = A + w[n]</span>, <span
class="math inline">w[n] \sim \mathcal{N}(0, \sigma_n^2)</span>, tada:
<span class="math display">
\hat{A} = \frac{\sum_{n=0}^{N-1}
\frac{x[n]}{\sigma_n^2}}{\sum_{n=0}^{N-1} \frac{1}{\sigma_n^2}}
</span> Merenja sa manjom varijansom dobijaju veći ponder.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako se procenjuje <span class="math inline">\theta</span> kad
signal ima poznatu komponentu <span
class="math inline">s</span>?</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">x = H\theta + s + w</span>,<br />
oduzimamo poznatu komponentu: <span class="math display">
x&#39; = x - s
</span> i računamo: <span class="math display">
\hat{\theta} = (H^T H)^{-1} H^T (x - s)
</span></p>
</div></details>
<hr />
<h2 data-number="5"
id="nlne-najbolji-linearan-nepomeren-estimator-blue"><span
class="header-section-number">5</span> NLNE – Najbolji linearan
nepomeren estimator (BLUE)</h2>
<h3 data-number="5.1" id="motivacija"><span
class="header-section-number">5.1</span> Motivacija</h3>
<details class="card">
<summary>
<strong>Zašto uopšte uvodimo NLNE/BLUE?</strong>
</summary>
<div class="card-body">
<p>Kada <span class="math inline">p(x;\theta)</span> nije poznata ili
klasični kriterijumi (CRDG, rizično-bayesovski) nisu primenljivi,
ograničavamo se na <strong>linearne</strong> estimatore i tražimo onaj
koji je <strong>nepomeren</strong> i ima <strong>minimalnu
varijansu</strong>.<br />
Usvajamo linearni oblik: <span class="math display">
\hat{\theta} = \sum_{n=0}^{N-1} a_n\, x[n] = \mathbf{a}^T \mathbf{x}.
</span></p>
<p><strong>Intuicija:</strong> dovoljno su nam <span
class="math inline">E[\mathbf{x}]</span> i <span
class="math inline">\mathrm{cov}(\mathbf{x})</span>; ne moramo znati
celu raspodelu <span class="math inline">p(x;\theta)</span>.</p>
</div></details>
<hr />
<h3 data-number="5.2" id="uslov-nepomerenosti-skalarni-slučaj"><span
class="header-section-number">5.2</span> Uslov nepomerenosti (skalarni
slučaj)</h3>
<details class="card">
<summary>
<strong>Koji je uslov nepomerenosti za <span
class="math inline">\hat{\theta}=\mathbf{a}^T\mathbf{x}</span>?</strong>
</summary>
<div class="card-body">
<p>Ako je model očekivanja <span class="math inline">E[\mathbf{x}] =
\theta\,\mathbf{s}</span> sa poznatim <span
class="math inline">\mathbf{s}</span>, tada: <span class="math display">
E[\hat{\theta}] = \theta \;\;\Longleftrightarrow\;\; \mathbf{a}^T
\mathbf{s} = 1.
</span></p>
</div></details>
<hr />
<h3 data-number="5.3" id="nalaženje-nlne-skalarni-slučaj"><span
class="header-section-number">5.3</span> Nalaženje NLNE (skalarni
slučaj)</h3>
<details class="card">
<summary>
<strong>Kako naći NLNE kada znamo <span
class="math inline">\mathrm{cov}(\mathbf{x})=\mathbf{C}</span> i <span
class="math inline">E[\mathbf{x}]=\theta\mathbf{s}</span>?</strong>
</summary>
<div class="card-body">
<p>Minimizujemo varijansu uz uslov nepomerenosti: <span
class="math display">
\min_{\mathbf{a}}\;\mathbf{a}^T \mathbf{C}\,\mathbf{a}\quad
\text{uz}\quad \mathbf{a}^T \mathbf{s}=1.
</span> Lagrandžijan: <span class="math display">
L(\mathbf{a},\lambda) = \mathbf{a}^T\mathbf{C}\mathbf{a} +
\lambda\,(\mathbf{a}^T\mathbf{s}-1).
</span> Rešenje: <span class="math display">
\mathbf{a}_{\text{opt}} = \frac{\mathbf{C}^{-1}\mathbf{s}}{\mathbf{s}^T
\mathbf{C}^{-1}\mathbf{s}},\qquad
\mathrm{var}(\hat{\theta}) = \frac{1}{\mathbf{s}^T
\mathbf{C}^{-1}\mathbf{s}}.
</span></p>
<p><strong>Komentar (intuicija):</strong> ovo je “pretezani korelator” –
prvo <strong>izbeljivanje</strong> (<span
class="math inline">\mathbf{C}^{-1}</span>), pa
<strong>usklađivanje</strong> sa <span
class="math inline">\mathbf{s}</span>. U belom šumu <span
class="math inline">\mathbf{C}=\sigma^2\mathbf{I}</span> se svodi na
<span class="math display">
\mathbf{a}_{\text{opt}}=\frac{\mathbf{s}}{\mathbf{s}^T\mathbf{s}},\quad
\hat{\theta}=\frac{\mathbf{s}^T\mathbf{x}}{\mathbf{s}^T\mathbf{s}}.
</span></p>
</div></details>
<hr />
<h3 data-number="5.4" id="kada-nlne-nije-adekvatan"><span
class="header-section-number">5.4</span> Kada NLNE nije adekvatan?</h3>
<details class="card">
<summary>
<strong>Postoji li situacija u kojoj linearni nepomeren estimator ne
može da pomogne?</strong>
</summary>
<div class="card-body">
<p>Da. Ako je, recimo, <span class="math inline">E[x[n]]=0</span>, a
parametar je <span
class="math inline">\theta=\mathrm{var}\{x[n]\}</span>, svaki linearni
estimator <span class="math inline">\sum a_n x[n]</span> je
<strong>nepomeren za nulu</strong>, pa ne možemo linearnim putem doći do
<span class="math inline">\theta</span>.<br />
<strong>Intuicija:</strong> kada je informacija o parametru
“kvadratična”, treba nelinearan pristup (npr. koristiti <span
class="math inline">x^2[n]</span>).</p>
</div></details>
<hr />
<h3 data-number="5.5"
id="vektorski-slučaj-blue-za-boldsymbolthetainmathbbrp"><span
class="header-section-number">5.5</span> Vektorski slučaj (BLUE za <span
class="math inline">\boldsymbol{\theta}\in\mathbb{R}^p</span>)</h3>
<details class="card">
<summary>
<strong>Kako izgleda linearni nepomeren estimator vektora?</strong>
</summary>
<div class="card-body">
<p>Uzimamo <span
class="math inline">\hat{\boldsymbol{\theta}}=\mathbf{A}\mathbf{x}</span>
i tražimo nepomerenost uz model <span
class="math inline">E[\mathbf{x}]=\mathbf{H}\boldsymbol{\theta}</span>:
<span class="math display">
E[\hat{\boldsymbol{\theta}}]=\boldsymbol{\theta}
\;\;\Longleftrightarrow\;\;
\mathbf{A}\mathbf{H}=\mathbf{I}.
</span> Komponentno: neka su <span
class="math inline">\mathbf{a}_i^T</span> vrste od <span
class="math inline">\mathbf{A}</span> i <span
class="math inline">\mathbf{h}_j</span> kolone od <span
class="math inline">\mathbf{H}</span>. Tada: <span class="math display">
\mathbf{a}_i^T\mathbf{h}_j = \delta_{ij}.
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kako naći <span class="math inline">\mathbf{A}</span> sa
minimalnom kovarijansom?</strong>
</summary>
<div class="card-body">
<p>Minimizujemo svaku <span
class="math inline">\mathrm{var}(\hat{\theta}_i)=[\mathbf{A}\mathbf{C}\mathbf{A}^T]_{ii}</span>
pod ograničenjem <span
class="math inline">\mathbf{A}\mathbf{H}=\mathbf{I}</span>.<br />
Rezultat (BLUE/GLS): <span class="math display">
\boxed{\;
\hat{\boldsymbol{\theta}} =
(\mathbf{H}^T\mathbf{C}^{-1}\mathbf{H})^{-1}\mathbf{H}^T\mathbf{C}^{-1}\mathbf{x},\qquad
\mathbf{C}_{\hat{\boldsymbol{\theta}}} =
(\mathbf{H}^T\mathbf{C}^{-1}\mathbf{H})^{-1}
\;}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kakva je veza sa NEMV (MVUE) u linearnom Gaussovom
modelu?</strong>
</summary>
<div class="card-body">
<p>Ako je <span
class="math inline">\mathbf{x}=\mathbf{H}\boldsymbol{\theta}+\mathbf{w}</span>
sa <span
class="math inline">\mathbf{w}\sim\mathcal{N}(\mathbf{0},\mathbf{C})</span>,
tada je BLUE iznad <strong>istovremeno i MVUE</strong>: <span
class="math display">
\text{NLNE} \equiv \text{NEMV} \quad (\text{u linearnom Gaussovom
modelu}).
</span></p>
<p><strong>Intuicija:</strong> Gaussov slučaj “zatvara krug”: linearni,
nepomeren i minimalne varijanse je ujedno i globalno optimalan među svim
nepomerenim.</p>
</div></details>
<hr />
<h2 data-number="6"
id="estimator-maksimalne-verodostojnosti-emv-mle"><span
class="header-section-number">6</span> Estimator maksimalne
verodostojnosti (EMV / MLE)</h2>
<h3 data-number="6.1" id="definicija-i-motivacija"><span
class="header-section-number">6.1</span> Definicija i motivacija</h3>
<details class="card">
<summary>
<strong>Šta je estimator maksimalne verodostojnosti (EMV)?</strong>
</summary>
<div class="card-body">
<p>EMV je estimator: <span class="math display">
\hat{\theta} = \arg\max_{\theta} p(x; \theta) = \arg\max_{\theta}
L(\theta) = \arg\max_{\theta} \ell(\theta),
</span> gde je <span class="math inline">L(\theta)</span> funkcija
verodostojnosti, a <span class="math inline">\ell(\theta)</span>
log-verodostojnost.</p>
<p><strong>Intuicija:</strong> biramo onu vrednost parametra <span
class="math inline">\theta</span> koja čini posmatrane podatke
<strong>najverovatnijim</strong>.</p>
</div></details>
<details class="card">
<summary>
<strong>Kada se koristi EMV?</strong>
</summary>
<div class="card-body">
<p>Koristimo ga:</p>
<ul>
<li><p>kada znamo tačnu formu raspodele <span class="math inline">p(x;
\theta)</span></p></li>
<li><p>kada NEMV (MVUE) ne možemo naći preko Cramer–Rao teoreme ili
drugih metoda</p></li>
</ul>
</div></details>
<hr />
<h3 data-number="6.2" id="primeri"><span
class="header-section-number">6.2</span> Primeri</h3>
<details class="card">
<summary>
<strong>Primer: konstanta <span class="math inline">A</span> u Gaussovom
šumu</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">x[n] \sim \mathcal{N}(A,
\sigma^2)</span>, log-verodostojnost je: <span class="math display">
\ell(A) = -\frac{N}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}
\sum_{n=0}^{N-1} (x[n] - A)^2
</span> Maksimizacijom po <span class="math inline">A</span> (<span
class="math inline">\ell&#39;(A) = 0</span>) dobijamo: <span
class="math display">
\hat{A}_{\text{EMV}} = \bar{x}
</span> što je isto kao i NEMV.</p>
<p><strong>Komentar:</strong> u ovom slučaju EMV je
<strong>efikasan</strong> i dostiže CRDG.</p>
</div></details>
<details class="card">
<summary>
<strong>Primer: Bernoulli raspodela</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">x[n] \sim
\text{Bernoulli}(\varphi)</span>,<br />
sa <span class="math inline">N_1 = \sum x[n]</span> i <span
class="math inline">N_0 = N - N_1</span>: <span class="math display">
L(\varphi) = \varphi^{N_1} (1 - \varphi)^{N_0}
</span> Maksimizacija daje: <span class="math display">
\hat{\varphi} = \frac{N_1}{N}
</span> <strong>Komentar:</strong> intuitivno, procena parametra <span
class="math inline">\varphi</span> je samo relativna učestalost
uspeha.</p>
</div></details>
<hr />
<h3 data-number="6.3" id="osobine-emv"><span
class="header-section-number">6.3</span> Osobine EMV</h3>
<details class="card">
<summary>
<strong>Koje su asimptotske osobine EMV-a?</strong>
</summary>
<div class="card-body">
<p>Ako su <span class="math inline">\ell&#39;(\theta)</span> i <span
class="math inline">\ell&#39;&#39;(\theta)</span> definisani, <span
class="math inline">I(\theta) \neq 0</span> i <span
class="math inline">E[\ell&#39;(\theta)] = 0</span>, tada: <span
class="math display">
\hat{\theta}_{\text{EMV}} \;\; \overset{a}{\sim} \;\; \mathcal{N} \left(
\theta, I^{-1}(\theta) \right), \quad N \to \infty
</span> odnosno EMV je:</p>
<ul>
<li><p>asimptotski <strong>normalan</strong></p></li>
<li><p>asimptotski <strong>nepomeren</strong></p></li>
<li><p>asimptotski <strong>efikasan</strong></p></li>
</ul>
</div></details>
<details class="card">
<summary>
<strong>Primer: konstanta <span class="math inline">A</span> u Gaussovom
šumu varijanse <span class="math inline">A</span></strong>
</summary>
<div class="card-body">
<p>Model: <span class="math inline">x[n] \sim \mathcal{N}(A,
A)</span><br />
Posle diferenciranja i rešavanja kvadratne jednačine: <span
class="math display">
\hat{A} = -\frac{1}{2} + \sqrt{\frac{1}{N} \sum_{n=0}^{N-1} x^2[n] +
\frac{1}{4}}
</span> Za <span class="math inline">N \to \infty</span>: <span
class="math display">
\hat{A} \; \overset{a}{\sim} \; \mathcal{N} \left( A,
\frac{A^2}{N\left(A + \frac12\right)} \right)
</span></p>
<p><strong>Komentar:</strong> ovde EMV <strong>nije</strong> linearna
funkcija uzoraka, pa se razlikuje od BLUE.</p>
</div></details>
<hr />
<h3 data-number="6.4" id="numerička-maksimizacija-verodostojnosti"><span
class="header-section-number">6.4</span> Numerička maksimizacija
verodostojnosti</h3>
<details class="card">
<summary>
<strong>Kako numerički maksimizirati verodostojnost?</strong>
</summary>
<div class="card-body">
<ol type="1">
<li><strong>Pretraga po mreži:</strong> za <span
class="math inline">\theta \in [a,b]</span> računamo <span
class="math inline">p(x; \theta)</span> u tačkama <span
class="math inline">a, a+\Delta\theta, \dots, b</span>.</li>
<li><strong>Newton–Raphson metoda:</strong> <span class="math display">
\theta_{k+1} = \theta_k -
\frac{\ell&#39;(\theta_k)}{\ell&#39;&#39;(\theta_k)}
</span> traži nulu prvog izvoda.</li>
<li><strong>Metoda skorovanja:</strong> <span class="math display">
\ell&#39;&#39;(\theta) \to E[\ell&#39;&#39;(\theta)] = -I(\theta), \quad
\theta_{k+1} = \theta_k + I^{-1}(\theta_k) \, \ell&#39;(\theta_k)
</span> ima manju zavisnost od konkretnog uzorka.</li>
</ol>
</div></details>
<hr />
<h2 data-number="7"
id="bayesovska-filozofija-i-emskg-posteriorna-sredina"><span
class="header-section-number">7</span> Bayesovska filozofija i EMSKG
(posteriorna sredina)</h2>
<h3 data-number="7.1" id="bayesovska-filozofija"><span
class="header-section-number">7.1</span> Bayesovska filozofija</h3>
<details class="card">
<summary>
<strong>Šta je osnovna razlika između klasičnog i Bayesovskog pristupa
estimaciji?</strong>
</summary>
<div class="card-body">
<ul>
<li><strong>Klasičan pristup</strong>: parametar <span
class="math inline">\theta</span> je determinističan ali nepoznat.</li>
<li><strong>Bayesovski pristup</strong>: parametar <span
class="math inline">\theta</span> se tretira kao slučajna promenljiva sa
apriornom raspodelom <span class="math inline">p(\theta)</span>.</li>
</ul>
<p><strong>Komentar</strong>: Bayesovska filozofija omogućava da u
estimaciju uključimo predznanje o parametru.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako se računa aposteriorna funkcija gustine verovatnoće <span
class="math inline">p(\theta|x)</span>?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
p(\theta | x) = \frac{p(x | \theta) p(\theta)}{p(x)}
</span></p>
<p>gde je: <span class="math display">
p(x) = \int p(x | \theta) p(\theta) \, d\theta
</span></p>
</div></details>
<hr />
<h3 data-number="7.2"
id="bayesovska-srednje-kvadratna-greška-bskg"><span
class="header-section-number">7.2</span> Bayesovska srednje-kvadratna
greška (BSKG)</h3>
<details class="card">
<summary>
<strong>Kako se definiše Bayesovska srednje-kvadratna greška?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\mathrm{Bmse}(\hat{\theta}) = \iint \left[ \theta - \hat{\theta}(x)
\right]^2 p(x,\theta) \, dx \, d\theta
</span></p>
<p><strong>Komentar</strong>: Za razliku od klasične SKG, BSKG se
“uprosečuje” po <span class="math inline">\theta</span>, pa ne zavisi od
njegove stvarne vrednosti.</p>
</div></details>
<hr />
<h3 data-number="7.3"
id="estimator-minimalne-srednje-kvadratne-greške-emskg"><span
class="header-section-number">7.3</span> Estimator minimalne
srednje-kvadratne greške (EMSKG)</h3>
<details class="card">
<summary>
<strong>Kako izgleda EMSKG estimator?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\hat{\theta} = \mathbb{E}[\theta | x]
</span></p>
<p><strong>Komentar</strong>: Ovo znači da je najbolja procena <span
class="math inline">\theta</span> u Bayesovskom smislu njegova uslovna
očekivana vrednost nakon posmatranja <span
class="math inline">x</span>.</p>
</div></details>
<hr />
<h3 data-number="7.4" id="primer-konstanta-u-belom-gaussovom-šumu"><span
class="header-section-number">7.4</span> Primer: konstanta u belom
Gaussovom šumu</h3>
<details class="card">
<summary>
<strong>Kako se dobija aposteriorna raspodela ako je <span
class="math inline">A \sim U[-A_0,A_0]</span> i <span
class="math inline">x|A \sim \mathcal{N}(A\mathbf{1}, \sigma^2
I)</span>?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
p(A|x) =
\begin{cases}
\frac{1}{c} \, \varphi\left(A; \bar{x}, \frac{\sigma^2}{N}\right), &amp;
|A| \leq A_0 \\
0, &amp; |A| &gt; A_0
\end{cases}
</span></p>
<p>gde je: <span class="math display">
c = \int_{-A_0}^{A_0} \varphi\left(A; \bar{x}, \frac{\sigma^2}{N}\right)
\, dA
</span></p>
<p><strong>Komentar</strong>: Ako je <span class="math inline">N</span>
veliko, varijansa <span class="math inline">\sigma^2/N \to 0</span>, pa
se raspodela koncentriše oko <span class="math inline">\bar{x}</span> i
apriorno znanje postaje nebitno. Estimacija je asimptotski
nepomerena.</p>
</div></details>
<hr />
<h3 data-number="7.5" id="izbor-apriorne-fgv"><span
class="header-section-number">7.5</span> Izbor apriorne FGV</h3>
<details class="card">
<summary>
<strong>Kako biramo apriornu raspodelu <span
class="math inline">p(\theta)</span>?</strong>
</summary>
<div class="card-body">
<ul>
<li><p><strong>Informativna</strong>: imamo predznanje (ograničenja,
tipične vrednosti)</p></li>
<li><p><strong>Neinformativna</strong>: nemamo predznanje (npr.
uniformna na opsegu)</p></li>
<li><p><strong>Konjugovana</strong>: bira se tako da je
<strong>aposteriorna</strong> iste familije kao apriorna → dobijamo
<strong>zatvorenu formu</strong></p></li>
</ul>
</div></details>
<hr />
<h3 data-number="7.6" id="bayesovski-vs.-frekventistički-pristup"><span
class="header-section-number">7.6</span> Bayesovski vs. frekventistički
pristup</h3>
<details class="card">
<summary>
<strong>Kada je bolji Bayesovski estimator?</strong>
</summary>
<div class="card-body">
<ul>
<li><p>Kada imaš pouzdano predznanje i/ili mali broj uzoraka</p></li>
<li><p>Daje stabilnije procene: koristi <span
class="math inline">p(\theta)</span> i “vuče” procenu ka realističnim
vrednostima</p></li>
</ul>
</div></details>
<details class="card">
<summary>
<strong>Kada je bolji frekventistički estimator?</strong>
</summary>
<div class="card-body">
<ul>
<li><p>Kada želiš neutralnost (bez apriornih pretpostavki)</p></li>
<li><p>Kada imaš mnogo podataka: asimptotski je nepomeren i efikasan bez
oslanjanja na <span class="math inline">p(\theta)</span></p></li>
</ul>
</div></details>
<hr />
<h3 data-number="7.7" id="osobine-gaussove-fgv"><span
class="header-section-number">7.7</span> Osobine Gaussove FGV</h3>
<details class="card">
<summary>
<strong>Koji je oblik aposteriorne raspodele za združeno normalne <span
class="math inline">x</span> i <span
class="math inline">\theta</span>?</strong>
</summary>
<div class="card-body">
<p>Ako je: <span class="math display">
\begin{bmatrix} x \\ \theta \end{bmatrix} \sim \mathcal{N}\left(
\begin{bmatrix} \mathbb{E}x \\ \mathbb{E}\theta \end{bmatrix},
\begin{bmatrix} C_x &amp; C_{x\theta} \\ C_{\theta x} &amp; C_\theta
\end{bmatrix} \right)
</span></p>
<p>onda: <span class="math display">
\mu_{\theta|x} = \mathbb{E}\theta + C_{\theta x} C_x^{-1} (x -
\mathbb{E}x)
</span> <span class="math display">
C_{\theta|x} = C_\theta - C_{\theta x} C_x^{-1} C_{x\theta}
</span></p>
<p><strong>Komentar:</strong> Uoči da je EMSKG inkrementalno linearna
funkcija od <span class="math inline">\theta</span>.</p>
</div></details>
<hr />
<h3 data-number="7.8" id="bayesovski-linearan-model"><span
class="header-section-number">7.8</span> Bayesovski linearan model</h3>
<details class="card">
<summary>
<strong>Koji su osnovni izrazi za Bayesovski linearan model <span
class="math inline">x = H\theta + w</span>?</strong>
</summary>
<div class="card-body">
<p>Za <span class="math inline">\theta \sim \mathcal{N}(\mu_\theta,
C_\theta)</span>, <span class="math inline">w \sim \mathcal{N}(0,
C_w)</span>:</p>
<p><span class="math display">
\mu_{\theta|x} = \mu_\theta + C_\theta H^T (H C_\theta H^T + C_w)^{-1}
(x - H\mu_\theta)
</span> <span class="math display">
C_{\theta|x} = C_\theta - C_\theta H^T (H C_\theta H^T + C_w)^{-1} H
C_\theta
</span></p>
</div></details>
<hr />
<h3 data-number="7.9"
id="alternativne-forme-za-c_thetax-i-mu_thetax"><span
class="header-section-number">7.9</span> Alternativne forme za <span
class="math inline">C_{\theta|x}</span> i <span
class="math inline">\mu_{\theta|x}</span></h3>
<details class="card">
<summary>
<strong>Kako izgleda alternativna forma za <span
class="math inline">C_{\theta|x}</span> pomoću leme o inverziji
matrice?</strong>
</summary>
<div class="card-body">
<p>Koristeći lemu o inverziji matrice: <span class="math display">
C_{\theta|x} = \left( C_\theta^{-1} + H^T C_w^{-1} H \right)^{-1}
</span> Ovo je ekvivalentno standardnoj formi, ali često olakšava
računanje kada su dimenzije <span class="math inline">H</span> i <span
class="math inline">C_w</span> nepovoljne.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda alternativna forma za <span
class="math inline">\mu_{\theta|x}</span> pomoću oznake <span
class="math inline">s</span>?</strong>
</summary>
<div class="card-body">
<p>Definišemo: <span class="math display">
s = H C_\theta H^T + C_w
</span> Tada: <span class="math display">
\mu_{\theta|x} = \mu_\theta + C_{\theta|x} H^T C_w^{-1} \left( x - H
\mu_\theta \right)
</span> Ova forma jasno pokazuje kako Bayesovski estimator koriguje
apriornu sredinu na osnovu reziduala <span class="math inline">x -
H\mu_\theta</span>.</p>
<p><strong>Intuicija:</strong></p>
<ul>
<li><strong>Lema o inverziji matrice</strong>: omogućava da biraš
redosled računanja i tako smanjiš složenost.<br />
</li>
<li><strong>Oznaka <span class="math inline">s</span></strong>: ponaša
se kao “Bayesovski pojačivač” koji odlučuje koliko poverenja da daš
novim merenjima u odnosu na apriorno verovanje.</li>
</ul>
</div></details>
<hr />
<h3 data-number="7.10" id="dodatna-pitanja"><span
class="header-section-number">7.10</span> Dodatna pitanja</h3>
<details class="card">
<summary>
<strong>Kako se osloboditi neželjenih parametara u Bayesovskom
okviru?</strong>
</summary>
<div class="card-body">
<p>Integracijom preko neželjenih parametara iz zajedničke gustine: <span
class="math display">
p(\theta | x) = \int p(\theta, \phi | x) \, d\phi
</span> gde je <span class="math inline">\phi</span> neželjeni
parametar.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako znamo da su <span class="math inline">x</span> i <span
class="math inline">\theta</span> združeno normalni u Gaussovskom
Bayesovskom modelu?</strong>
</summary>
<div class="card-body">
<p>Ako je vektor <span class="math display">
\begin{bmatrix} x \\ \theta \end{bmatrix}
</span> linearno zavisan od nezavisnih normalnih vektora <span
class="math inline">\theta</span> i <span class="math inline">w</span>,
tada je on sam normalno raspodeljen (svojstvo linearne kombinacije
normalnih).</p>
</div></details>
<hr />
<h2 data-number="8" id="opšta-bayesovska-estimacija"><span
class="header-section-number">8</span> Opšta Bayesovska estimacija</h2>
<h3 data-number="8.1" id="osnovni-pojmovi"><span
class="header-section-number">8.1</span> Osnovni pojmovi</h3>
<details class="card">
<summary>
<strong>Šta je greška estimacije i kako se definiše cena?</strong>
</summary>
<div class="card-body">
<p>Greška estimacije: <span class="math inline">\varepsilon = \theta -
\hat{\theta}</span><br />
Cena <span class="math inline">C(\varepsilon)</span> određuje težinu
greške:</p>
<ul>
<li><p><strong>Kvadratna:</strong> <span
class="math inline">C(\varepsilon) = \varepsilon^2</span></p></li>
<li><p><strong>Apsolutna:</strong> <span
class="math inline">C(\varepsilon) = |\varepsilon|</span></p></li>
<li><p><strong>“0/1” cena:</strong><br />
<span class="math display">
C(\varepsilon) =
\begin{cases}
0, &amp; |\varepsilon| \le \delta \\
1, &amp; |\varepsilon| &gt; \delta
\end{cases}
</span></p></li>
</ul>
</div></details>
<details class="card">
<summary>
<strong>Kako se definiše rizik i optimalni estimator u Bayesovskom
smislu?</strong>
</summary>
<div class="card-body">
<p>Rizik: <span class="math display">
R = E\{ C(\varepsilon) \} =
\int \left[ \int C(\theta - \hat{\theta})\, p(\theta | x) \, d\theta
\right] p(x) \, dx
</span> Optimalni estimator minimizuje rizik: <span
class="math display">
\hat{\theta}^* = \arg\min_{\hat{\theta}} \int C(\theta - \hat{\theta})
\, p(\theta|x) \, d\theta
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda optimalni estimator za kvadratnu cenu?</strong>
</summary>
<div class="card-body">
<p>Za kvadratnu cenu, optimalan je EMSKG: <span class="math display">
\hat{\theta}_{\mathrm{emskg}} = E\{\theta | x\}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda optimalni estimator za apsolutnu cenu?</strong>
</summary>
<div class="card-body">
<p>Za apsolutnu cenu, uslov optimalnosti: <span class="math display">
\int_{-\infty}^{\hat{\theta}} p(\theta|x)\, d\theta =
\int_{\hat{\theta}}^{\infty} p(\theta|x)\, d\theta
</span> Optimalni estimator je <strong>medijana</strong> aposteriorne
raspodele.</p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda optimalni estimator za 0/1 cenu?</strong>
</summary>
<div class="card-body">
<p>Za malo <span class="math inline">\delta</span>: <span
class="math display">
\hat{\theta}^* = \arg\max_{\theta} p(\theta | x) = \arg\max_{\theta}
p(x|\theta) p(\theta)
</span> Ovo je <strong>MAP</strong> (maximum a posteriori)
estimator.</p>
</div></details>
<hr />
<h3 data-number="8.2" id="osobine-emskg"><span
class="header-section-number">8.2</span> Osobine EMSKG</h3>
<details class="card">
<summary>
<strong>Šta je invarijantnost EMSKG-a?</strong>
</summary>
<div class="card-body">
<p>Ako je <span class="math inline">\alpha = A\theta + b</span>, onda:
<span class="math display">
\hat{\alpha} = E\{\alpha|x\} = A \hat{\theta} + b
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda aditivnost EMSKG-a u Gaussovskom modelu za
nezavisne <span class="math inline">x_1</span> i <span
class="math inline">x_2</span>?</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\hat{\theta} = E\theta + \sum_{i=1}^2 C_{\theta x_i} C_{x_i}^{-1} (x_i -
E x_i)
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Čemu je jednaka EMSKG za <span
class="math inline">\theta_i</span>?</strong>
</summary>
<div class="card-body">
<p>Za komponentu <span class="math inline">\theta_i</span> vektora <span
class="math inline">\theta</span>, srednja kvadratna greška estimatora
(EMSKG) je jednaka:</p>
<p><span class="math display">
\text{EMSKG}(\theta_i) = \mathrm{var}(\hat{\theta}_i) + \left[
\mathrm{bias}(\hat{\theta}_i) \right]^2
</span></p>
<p>Ovo pokazuje da se ukupna greška sastoji iz varijanse estimatora i
kvadrata pomaka.</p>
</div></details>
<details class="card">
<summary>
<strong>Da li je EMSKG nepomeren estimator?</strong>
</summary>
<div class="card-body">
<p>Da, EMSKG je nepomeren: <span class="math display">
E\{\varepsilon\} = E\{\theta - E_{\theta|x}(\theta)\} = 0
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Kako izgleda varijansa greške kod EMSKG-a?</strong>
</summary>
<div class="card-body">
<p>U skalaru: <span class="math display">
\mathrm{var}(\hat{\theta}) = \mathrm{Bmse}(\hat{\theta})
</span> U vektorskom obliku: <span class="math display">
M_{\hat{\theta}} = E_x \, C_{\theta|x}
</span></p>
</div></details>
<details class="card">
<summary>
<strong>Ako su <span class="math inline">X</span> i <span
class="math inline">\theta</span> združeno normalne, kakva je raspodela
greške <span class="math inline">\varepsilon</span>?</strong>
</summary>
<div class="card-body">
<p>Ako su <span class="math inline">X</span> i <span
class="math inline">\theta</span> združeno normalne slučajne
promenljive, tada je i greška estimacije <span
class="math inline">\varepsilon = \hat{\theta} - \theta</span> takođe
normalno distribuirana.</p>
<p><strong>Intuicija:</strong><br />
- Linearna kombinacija normalnih slučajnih promenljivih je opet
normalna.<br />
- Kako je <span class="math inline">\varepsilon</span> linearna
kombinacija <span class="math inline">X</span> i <span
class="math inline">\theta</span>, normalnost se čuva.</p>
</div></details>
<hr />
<h3 data-number="8.3" id="osobine-mape"><span
class="header-section-number">8.3</span> Osobine MAPE</h3>
<details class="card">
<summary>
<strong>Šta je prednost MAPE u odnosu na EMSKG?</strong>
</summary>
<div class="card-body">
<p>MAPE ne zahteva integraciju, već samo maksimizaciju.</p>
</div></details>
<details class="card">
<summary>
<strong>Da li je MAPE invarijantan na transformacije
parametara?</strong>
</summary>
<div class="card-body">
<p>Ne, osim za linearne transformacije <span class="math inline">\alpha
= a\theta + b</span>.</p>
</div></details>
<hr />
<h2 data-number="9" id="linearna-bayesovska-estimacija-lbe"><span
class="header-section-number">9</span> Linearna Bayesovska estimacija
(LBE)</h2>
<h3 data-number="9.1" id="definicija-i-klasa-linearnog-estimatora"><span
class="header-section-number">9.1</span> Definicija i klasa linearnog
estimatora</h3>
<details class="card">
<summary>
<strong>Šta je klasa inkrementalno linearnih estimatora i šta je
LEMSKG?</strong>
</summary>
<div class="card-body">
<p>Klasa: <span class="math display">
\hat{\theta}=\sum_{n=0}^{N-1} a_n\,x[n] + a_N \;=\; \mathbf{a}^T
\mathbf{x} + a_N.
</span> <strong>LEMSKG</strong> (linearni estimator minimalne SKG)
rešava: <span class="math display">
(\mathbf{a},a_N)=\arg\min_{\mathbf{a},a_N}
E\!\left[(\theta-\mathbf{a}^T\mathbf{x}-a_N)^2\right].
</span> <strong>Napomena:</strong> LEMSKG je generalno suboptimalan u
odnosu na nelinearni Bayesovski MMSE.</p>
</div></details>
<h3 data-number="9.2" id="optimalni-koeficijenti-lemskg"><span
class="header-section-number">9.2</span> Optimalni koeficijenti
LEMSKG</h3>
<details class="card">
<summary>
<strong>Kako se računaju optimalni koeficijenti <span
class="math inline">\mathbf{a}</span> i <span
class="math inline">a_N</span>?</strong>
</summary>
<div class="card-body">
<p>Ako su <span class="math inline">E\{\theta\}</span> i <span
class="math inline">E\{\mathbf{x}\}</span> opšti (nenulta srednja):
<span class="math display">
\frac{\partial\,\mathrm{Bmse}(\hat{\theta})}{\partial a_N}=0 \Rightarrow
a_N=E\{\theta\}-\mathbf{a}^T E\{\mathbf{x}\}.
</span> Minimizacija po <span class="math inline">\mathbf{a}</span> daje
kvadratnu formu: <span class="math display">
E\!\big[(\mathbf{a}^T(\mathbf{x}-E\mathbf{x})-(\theta-E\theta))^2\big]
= \mathbf{a}^T \mathbf{C}_x\,\mathbf{a}-2\,\mathbf{c}_{\theta
x}^T\mathbf{a}+\mathrm{var}(\theta),
</span> pa iz <span class="math inline">\nabla_{\mathbf{a}}=0</span>
sledi <span class="math display">
\boxed{\;\mathbf{a}= \mathbf{C}_x^{-1}\mathbf{c}_{x\theta}\;}
</span> i na kraju <span class="math display">
\boxed{\;\hat{\theta}=E\{\theta\}+\mathbf{c}_{\theta
x}\,\mathbf{C}_x^{-1}\,(\mathbf{x}-E\{\mathbf{x}\})\;}
</span> uz <span class="math display">
\boxed{\;\mathrm{Bmse}(\hat{\theta})=\mathrm{var}(\theta)-\mathbf{c}_{\theta
x}\,\mathbf{C}_x^{-1}\mathbf{c}_{x\theta}\;}
</span></p>
</div></details>
<h3 data-number="9.3" id="geometrijska-interpretacija"><span
class="header-section-number">9.3</span> Geometrijska
interpretacija</h3>
<details class="card">
<summary>
<strong>Kako izgleda geometrijska interpretacija LEMSKG-a?</strong>
</summary>
<div class="card-body">
<p>Uz centriranje <span
class="math inline">\theta&#39;=\theta-E\theta,\;
\mathbf{x}&#39;=\mathbf{x}-E\mathbf{x}</span>, prostor slučajnih
promenljivih nulte srednje je vektorski prostor sa skalarom <span
class="math inline">\langle u,v\rangle=E(uv)</span>. Optimalno <span
class="math inline">\hat{\theta}</span> je <strong>projekcija</strong>
<span class="math inline">\theta</span> na potprostor koji generišu
opservacije: <span class="math display">
\frac{d}{da}E(\theta-a x)^2=0 \Rightarrow
a=\frac{\langle\theta,x\rangle}{\langle x,x\rangle},\quad
\hat{\theta}=\mathrm{proj}_{x}\theta.
</span> <strong>Intuicija:</strong> greška je ortogonalna prostoru
“objašnjenom” merenjima.</p>
</div></details>
<h3 data-number="9.4" id="princip-ortogonalnosti"><span
class="header-section-number">9.4</span> Princip ortogonalnosti</h3>
<details class="card">
<summary>
<strong>Koji je princip ortogonalnosti i šta iz njega sledi?</strong>
</summary>
<div class="card-body">
<p>Za <span
class="math inline">\hat{\theta}=\mathbf{a}^T\mathbf{x}</span> (nulte
srednje): <span class="math display">
\varepsilon=\theta-\hat{\theta} \;\perp\; \{x[0],\dots,x[N-1]\}
</span> odakle sledi sistem normalnih jednačina <span
class="math inline">\mathbf{C}_x\,\mathbf{a}=\mathbf{c}_{x\theta}</span>,
tj. <span
class="math inline">\mathbf{a}=\mathbf{C}_x^{-1}\mathbf{c}_{x\theta}</span>,
i <span
class="math inline">\;\mathrm{Bmse}(\hat{\theta})=\mathrm{var}(\theta)-\mathbf{c}_{\theta
x}\,\mathbf{C}_x^{-1}\mathbf{c}_{x\theta}\;</span></p>
</div></details>
<h3 data-number="9.5" id="vektorski-lemskg"><span
class="header-section-number">9.5</span> Vektorski LEMSKG</h3>
<details class="card">
<summary>
<strong>Kako izgleda vektorska forma LEMSKG-a i njegova
kovarijansa?</strong>
</summary>
<div class="card-body">
<p>Za vektorski parametar: <span class="math display">
\boxed{\;\hat{\boldsymbol{\theta}}=E\{\boldsymbol{\theta}\}+
\mathbf{C}_{\theta x}\,\mathbf{C}_x^{-1}(\mathbf{x}-E\{\mathbf{x}\})\;}
</span> <span class="math display">
\boxed{\;\mathbf{M}_{\hat{\boldsymbol{\theta}}}=\mathrm{Bmse}(\hat{\boldsymbol{\theta}})=
\mathbf{C}_\theta-\mathbf{C}_{\theta
x}\,\mathbf{C}_x^{-1}\mathbf{C}_{x\theta}\;}.
</span> <strong>Invarijantnost:</strong> za <span
class="math inline">\boldsymbol{\alpha}=A\boldsymbol{\theta}+b</span>
važi <span
class="math inline">\hat{\boldsymbol{\alpha}}=A\hat{\boldsymbol{\theta}}+b</span>.</p>
</div></details>
<hr />
<h2 data-number="10" id="kalmanov-filter-kf"><span
class="header-section-number">10</span> Kalmanov filter (KF)</h2>
<ul>
<li><strong>Model:</strong> Gauss–Markov stanje + BGŠ merenja.</li>
<li><strong>Rekurzije (skalar):</strong>
<ul>
<li>Predikcija: <span class="math inline">\hat s_{n|n-1}=a\,\hat
s_{n-1|n-1}</span>, <span class="math inline">M_{n|n-1}=a^2
M_{n-1|n-1}+\sigma_u^2</span>.</li>
<li>Pojačanje: <span class="math inline">K_n=
M_{n|n-1}/(\sigma_w^2+M_{n|n-1})</span>.</li>
<li>Ažuriranje: <span class="math inline">\hat s_{n|n}=\hat
s_{n|n-1}+K_n\,(x_n-\hat s_{n|n-1})</span>, <span
class="math inline">M_{n|n}=(1-K_n)M_{n|n-1}</span>.</li>
</ul></li>
<li><strong>Zašto:</strong> optimalna sekvencijalna fuzija informacija
(EMSKG=LEMSKG u Gaussovom slučaju); osnov za praćenje, navigaciju,
komunikacije.</li>
<li><strong>Primer:</strong> praćenje sporog signala u šumu; glatkoća vs
odziv zavisi od <span
class="math inline">\sigma_u^2,\sigma_w^2</span>.</li>
</ul>
<hr />
<h2 data-number="11" id="pregled-i-povezanost-formula"><span
class="header-section-number">11</span> Pregled i povezanost
formula</h2>
<h3 data-number="11.1" id="verodostojnost-i-log-verodostojnost"><span
class="header-section-number">11.1</span> Verodostojnost i
log-verodostojnost</h3>
<details class="card">
<summary>
<strong>Formule</strong>
</summary>
<div class="card-body">
<p><span class="math display">
L(\theta)=p(x;\theta),\qquad \ell(\theta)=\ln p(x;\theta)
</span></p>
<p><strong>Napomene</strong></p>
<ul>
<li>Aditivnost za IID uzorke: <span
class="math inline">\ell(\theta)=\sum_{n}\ln
p(x[n];\theta)</span>.<br />
</li>
</ul>
</div></details>
<hr />
<h3 data-number="11.2" id="fišerova-informacija-1"><span
class="header-section-number">11.2</span> Fišerova informacija</h3>
<details class="card">
<summary>
<strong>Skalarni slučaj</strong>
</summary>
<div class="card-body">
<p><span class="math display">
I(\theta) = -E\!\left[\frac{\partial^2}{\partial\theta^2}\ln
p(x;\theta)\right]
= E\!\left[\left(\frac{\partial}{\partial\theta}\ln
p(x;\theta)\right)^{\!2}\right]
</span></p>
<p><strong>Zašto su iste?</strong><br />
Pod <strong>uslovom regularnosti</strong> (može se zameniti redosled
izvoda i integrala; skup podrške ne zavisi od <span
class="math inline">\theta</span>), važi <span
class="math inline">E[\partial_\theta \ln p]=0</span>, pa su dve
definicije ekvivalentne.</p>
</div></details>
<details class="card">
<summary>
<strong>Matrica Fisherove informacije (vektorski parametar)</strong>
</summary>
<div class="card-body">
<p><span class="math display">
I(\theta)]_{ij}
= -E\!\left[\frac{\partial^2}{\partial\theta_i\partial\theta_j}\ln
p(x;\theta)\right]
= E\!\left[\partial_{\theta_i}\ln p \;\partial_{\theta_j}\ln p\right
</span></p>
</div></details>
<hr />
<h3 data-number="11.3" id="crdg-cramérrao-donja-granica-1"><span
class="header-section-number">11.3</span> CRDG – Cramér–Rao Donja
Granica</h3>
<details class="card">
<summary>
<strong>Skalar</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\mathrm{var}(\hat{\theta}) \;\ge\; \frac{1}{I(\theta)}
</span></p>
<p><strong>Vektor</strong></p>
<p><span class="math display">
C_{\hat{\theta}} - I(\theta)^{-1} \;\succeq\; 0
</span></p>
<p><strong>Efikasnost i uslov jednakosti</strong></p>
<p><span class="math display">
\nabla_\theta \ln p(x;\theta) \;=\; I(\theta)\,[g(x)-\theta]
\;\Rightarrow\; C_{\hat{\theta}}=I^{-1}(\theta)
</span></p>
<p><strong>Transformacija parametara</strong></p>
<p><span class="math display">
\alpha=a(\theta),\quad C_{\hat{\alpha}} \;\succeq\;
J(a)\,I^{-1}(\theta)\,J(a)^{T},\quad
J(a)=\frac{\partial a}{\partial\theta}
</span></p>
</div></details>
<hr />
<h3 data-number="11.4"
id="nlne-blue-best-linear-unbiased-i-lemskg-linearni-mmse"><span
class="header-section-number">11.4</span> NLNE / BLUE (best linear
unbiased) i LEMSKG (linearni MMSE)</h3>
<details class="card">
<summary>
<strong>Skalarni BLUE (poznat <span class="math inline">E[x]=\theta
s</span>, <span class="math inline">\mathrm{cov}(x)=C</span>)</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\hat{\theta}=\frac{s^{T}C^{-1}x}{s^{T}C^{-1}s},\qquad
\mathrm{var}(\hat{\theta})=\frac{1}{s^{T}C^{-1}s}
</span></p>
<p>U belom šumu <span class="math inline">C=\sigma^2 I</span>: <span
class="math inline">\hat{\theta}=\dfrac{s^T x}{s^T s}</span>.</p>
</div></details>
<details class="card">
<summary>
<strong>Vektorski BLUE/GLS (model <span
class="math inline">x=H\theta+w</span>, <span
class="math inline">\mathrm{cov}(w)=C_w</span>)</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\hat{\theta}=(H^T C_w^{-1}H)^{-1}H^TC_w^{-1}x,\qquad
C_{\hat{\theta}}=(H^T C_w^{-1}H)^{-1}
</span></p>
<p>Uslov nepomerenosti: <span class="math inline">A H=I</span> za <span
class="math inline">\hat{\theta}=A x</span>.</p>
</div></details>
<details class="card">
<summary>
<strong>LEMSKG (generalni linearni MMSE)</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\hat{\theta}=E\{\theta\}+C_{\theta x}\,C_x^{-1}(x-E\{x\}),\qquad
\mathrm{Bmse}=\mathrm{var}(\theta)-C_{\theta x}\,C_x^{-1}C_{x\theta}
</span></p>
<p>Geometrija: <span class="math inline">\hat{\theta}</span> je
<strong>projekcija</strong> <span class="math inline">\theta</span> na
prostor generisan <span class="math inline">x</span>.<br />
<strong>Veza sa Gauss–Bayes:</strong> isti izrazi važe i u linearnom
Gaussovom modelu.</p>
</div></details>
<hr />
<h3 data-number="11.5"
id="linearni-gaussovski-lg-bayes-model-posterior"><span
class="header-section-number">11.5</span> Linearni Gaussovski (LG) Bayes
model – posterior</h3>
<details class="card">
<summary>
<strong>Posteriorna kovarijansa i sredina (<span
class="math inline">x=H\theta+w</span>, <span
class="math inline">\theta\!\sim\!N(\mu_\theta,C_\theta)</span>, <span
class="math inline">w\!\sim\!N(0,C_w)</span>)</strong>
</summary>
<div class="card-body">
<p><span class="math display">
C_{\theta|x} = \big(C_\theta^{-1}+H^T C_w^{-1} H\big)^{-1}
</span> <span class="math display">
\mu_{\theta|x}=\mu_\theta+C_{\theta|x} H^T C_w^{-1}\,(x-H\mu_\theta)
</span></p>
<p><strong>Ekvivalentan oblik (lema o inverziji matrice)</strong></p>
<p><span class="math display">
C_{\theta|x}=C_\theta - C_\theta H^T\,(H C_\theta H^T + C_w)^{-1} H
C_\theta
</span></p>
</div></details>
<hr />
<h3 data-number="11.6" id="mle-emv"><span
class="header-section-number">11.6</span> MLE (EMV)</h3>
<details class="card">
<summary>
<strong>Definicija i osobine</strong>
</summary>
<div class="card-body">
<p><span class="math display">
\hat{\theta}_{\text{MLE}}=\arg\max_\theta p(x;\theta)
=\arg\max_\theta \ell(\theta),\qquad
\ell&#39;(\hat{\theta})=0
</span></p>
<p>Asimptotski (pod standardnim uslovima):<br />
<span class="math inline">\hat{\theta}_{\text{MLE}} \overset{a}{\sim}
N(\theta,\, I^{-1}(\theta))</span> — <strong>normalan, nepomeren i
efikasan</strong> za <span class="math inline">N\to\infty</span>.</p>
</div></details>
<details class="card">
<summary>
<strong>Primeri i numerika</strong>
</summary>
<div class="card-body">
<ul>
<li>Bernoulli: <span
class="math inline">\hat{\varphi}=\dfrac{N_1}{N}</span> (moda
verovatnoće).<br />
</li>
<li>Gauss <span class="math inline">A</span> poznata <span
class="math inline">\sigma^2</span>: <span
class="math inline">\hat{A}=\bar{x}</span>.<br />
</li>
<li>Newton–Raphson: <span
class="math inline">\theta_{k+1}=\theta_k-\dfrac{\ell&#39;(\theta_k)}{\ell&#39;&#39;(\theta_k)}</span>.<br />
</li>
<li>Skorovanje: <span class="math inline">\theta_{k+1}=\theta_k +
I^{-1}(\theta_k)\,\ell&#39;(\theta_k)</span>.</li>
</ul>
</div></details>
<hr />
<h3 data-number="11.7"
id="bayes-posterior-rizik-i-optimalni-estimatori"><span
class="header-section-number">11.7</span> Bayes: posterior, rizik i
optimalni estimatori</h3>
<details class="card">
<summary>
<strong>Posterior i Bayesov rizik</strong>
</summary>
<div class="card-body">
<p><span class="math display">
p(\theta|x)=\frac{p(x|\theta)\,p(\theta)}{p(x)},\qquad
R=E\{C(\theta-\hat{\theta})\}=\int\!\Big[\int
C(\theta-\hat{\theta})\,p(\theta|x)\,d\theta\Big]p(x)\,dx
</span></p>
<p><strong>Optimalni estimatori</strong></p>
<ul>
<li><p><strong>Kvadratna cena:</strong> <span
class="math inline">\Rightarrow</span> <strong>EMSKG</strong>: <span
class="math inline">\hat{\theta}=E\{\theta|x\}</span></p></li>
<li><p><strong>Apsolutna cena:</strong> <span
class="math inline">\Rightarrow</span> <strong>medijana</strong>
posteriora</p></li>
<li><p><strong>0/1 cena:</strong> (mali <span
class="math inline">\delta</span>) <span
class="math inline">\Rightarrow</span> <strong>MAP</strong> <span
class="math inline">\arg\max_\theta p(\theta|x)</span></p></li>
</ul>
</div></details>
<details class="card">
<summary>
<strong>Primer (apriorna uniformna) i “zatvorena forma”</strong>
</summary>
<div class="card-body">
<p>Uniformna apriorna na opsegu <span
class="math inline">[-\!A_0,A_0]</span> i Gaussov likelihood daju
<strong>odsečenu normalnu</strong> za <span
class="math inline">p(A|x)</span> sa modom u <span
class="math inline">\bar{x}</span>; za velika <span
class="math inline">N</span> posterior se koncentriše oko <span
class="math inline">\bar{x}</span>. “Zatvorena forma” = eksplicitna
formula bez numeričke integracije.</p>
</div></details>
<hr />
<h3 data-number="11.8"
id="veze-između-pristupa-blue-mvue-bayes-mle"><span
class="header-section-number">11.8</span> Veze između pristupa (BLUE,
MVUE, Bayes, MLE)</h3>
<details class="card">
<summary>
<strong>Kada su iste / kako prelaziš</strong>
</summary>
<div class="card-body">
<ul>
<li><strong>LG Gauss</strong>: BLUE <span class="math inline">=</span>
MVUE <span class="math inline">=</span> Bayes MMSE <span
class="math inline">=</span> LEMSKG (isti izrazi za <span
class="math inline">\mu_{\theta|x}</span>, <span
class="math inline">C_{\theta|x}</span>).<br />
</li>
<li><strong>Asimptotika</strong>: MLE <span
class="math inline">\to</span> efikasan i (asymp.) nepomeren <span
class="math inline">\Rightarrow</span> dostiže CRDG za <span
class="math inline">N\to\infty</span>.<br />
</li>
<li><strong>Bez dobrog priora i puno podataka</strong>: Bayes MMSE <span
class="math inline">\approx</span> MVUE/MLE.</li>
</ul>
</div></details>
<hr />
<p><a href="#top" class="back-to-top">⇧</a></p>
<!-- cache-bust 20250813221544 --></body>
</html>
